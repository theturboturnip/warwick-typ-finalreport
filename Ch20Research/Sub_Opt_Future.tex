\subsection{Future Work}
\label{sec:FutureOptimization}
Along with the items mentioned in the previous section, there are some optimizations planned to be implemented over the Christmas break and during Term 2.

CUDA devices are split into many threads, which are split into groups of 32 that are executed concurrently as a warp\cite{tool:CUDAProgrammingV1}.
If the threads in a warp attempt to access multiple words in the same cache line, the access is \textit{coalesced}\cite{NVIDIAHowBlog} and only one cache line needs to be fetched for the warp to continue.
Otherwise if the accesses all touch different cache lines, every cache line needs to be fetched before execution can continue for any of the threads.
The CUDA program attempts to arrange the threads such that they coalesce accesses, but it has not been verified to work yet.

% Const Restrict Pointers for __ldg (https://dl.acm.org/doi/pdf/10.1145/3238147.3241533 tries to do this automatically and found a perf boost, can cite that or cite other papers on it).
% \todomark{This seems more like a statement of an optimization than a "Future Work".}
The CUDA C Programming Guide\cite{NVIDIAGlobalGuide} states that read-only memory can be read into a special data cache using the \texttt{\_\_ldg()} intrinsic.
The compiler may insert this automatically when it detects that data must be read-only.
The use of \texttt{const} and \texttt{\_\_restrict\_\_} qualifiers on pointers that are read-only is encouraged to make read-only data obvious.
In \cite{10.1145/3238147.3241533} it was found that introducing these qualifiers where possible led to large speedups in pointer heavy applications, and while our case may not use many pointers this should still be implemented wherever possible.
In the CUDA implementation templates for input and output matrices are used that include these qualifiers automatically, and all kernels are assumed to restrict all pointer arguments.
However it has yet to be verified that \texttt{\_\_ldg()} is inserted in the correct places, which should be done in the future.

% GPU Vectorization (cite).
The ACA solution used Intel AVX and SSE instructions\cite{IntelCorporationIntroductionExtensions} to calculate four Poisson values at once\footnote{Vectors of eight were tried but were found to be slower than four.}.
Each CUDA core of a GPU has access to four-element vectors without any extensions, so this vectorization can be extended per CUDA core. 
This has not been implemented in the CUDA program, but is a future extension to test.
This may or may not actually speed up computation, as the memory bandwidth would be quadrupled and the computation is already excessively parallel.

% Mention parallelization with OpenMP
% did already?

% Highly optimized reductions (cite).
Calculating the simulation timestep and calculating the residual for a Poisson iteration both require a reduction over large blocks of data.
Highly parallel GPU optimizations have already been studied extensively, so it should be trivial to implement a very fast generic reduction kernel.
In \cite{CUDAParallelReduction} seven kernels are described, in ascending order of speed.
Currently the CUDA program uses the second kernel model, and this is planned to be moved up to the seventh kernel in the future.
