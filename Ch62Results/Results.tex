% !TEX root =  ../FinalReport.tex

\newcommand{\npoisson}{N_{Poisson}}

\chapter{Results}\label{sec:Results}

\section{Simulation}
\todomark{Reiterate reqs?}

\subsection{Speed}\label{sec:Results:Sim:Speed}
Each tick of a simulation performs the same amount of work, so it was hypothesized that the number of ticks per second would be a suitable metric for measuring simulation speed.
To verify this, the initial ACA input was simulated with the CUDA and CPU backends for 10, 25, and 50 simulation-seconds with $\npoisson = 100$.
As shown in \todoref{Simulation ticks/time for ACA input}, elapsed real-time is directly proportional to number of ticks executed with R-values of $R^2_{CPU} = 0.9999$ and $R^2_{CUDA} = 0.9996$.
This proves that ticks-per-second can be used to accurately compare CUDA/CPU performance.

The ticks-per-second for CPU and CUDA are shown in \todoref{Simulation ticks/s for ACA input}, varying $\npoisson \in {100, 200, 300, 1000}$.
As expected the ticks-per-second decreases as more iterations are added, and CUDA is consistently faster than the CPU.
When normalized relative to the CPU speed (\todoref{Simulation ticks/s for ACA input rel. to CPU}), CUDA is shown to consistently have \~2.6x the ticks-per-second of the CPU, i.e. CUDA is consistently 2.6x faster than the CPU. 
The Poisson stage is likely less sped up than the others, as the speedup for $\npoisson{} = 1000$ (where the Poisson stage dominates) is less than that for lower $\npoisson{}$ values.

To determine how the implementations scale with grid resolution, a simple simulation with no obstacle squares was performed for resolutions between 260~x~130 and 4096~x~2048.
As the amount of work per simulation tick varies with grid size, each tick is multiplied by the number of grid cells and number of Poisson iterations to determine the number of operations (\si{op}s) executed, and  mega-operations-per-second (\si{\mega\op\per\second}) is used to measure performance.
The increasing size is measured in the total amount of memory (in \si{\mega\byte}) required for a full Poisson red/black iteration, equal to three full size matrices: the pressure matrix, the $\beta$ matrix, and the $rhs$ matrix.
\todoref{Throughput vs. Poisson Data Size} shows \si{\mega\op\per\second} for CUDA and CPU against the memory required in \si{\mega\byte}.

Initially, the biggest surprise in \todoref{throughput vs. poisson data size} was the CPU performing better than CUDA between 4-\todomark{?\SI{}{\mega\byte}}.
Other interesting points were the linear increase in CUDA throughput between 0.4-\SI{1.2}{\mega\byte}, and the performance plateau from \SI{4}{\mega\byte} onwards (CUDA) and \todomark{??\SI{}{\mega\byte}} onwards (CPU).
These are due to multiple factors, and are best explained by separating the CUDA graph into stages (\todoref{figure with subfigures for each separate state}).

\todoref{Inital climb state} shows \si{\mega\op\per\second} increasing almost linearly before plateauing at 1.2-\SI{1.5}{\mega\byte}.
This is due to the GPU's parallelization not being fully utilized.
This data was measured on a GTX 1080 which has 2560 CUDA cores separated into 20 Streaming Multiprocessors of 128 cores each\todocite{https://international.download.nvidia.com/geforce-com/international/pdfs/GeForce_GTX_1080_Whitepaper_FINAL.pdf}. % \todocite{https://www.nvidia.com/en-gb/geforce/graphics-cards/geforce-gtx-1080/specifications/}
Each Multiprocessor can execute 2048~threads at once \todocite{https://docs.nvidia.com/cuda/cuda-occupancy-calculator/index.html}\todomark{(find better citation?)}, thus the GPU can execute at most 40,960~threads in parallel.
The first datapoint uses a 260~x~130 grid with only 33,800~cells, thus 16,900 threads per Poisson color stage\footnote{The red and black stages only write to the 1/2 of the grid corresponding to their color.}, so the GPU isn't saturated and isn't producing as many outputs as possible per second.
This is also the case for the second datapoint, with 36,100 threads per color stage.
Beyond this point, the GPU is saturated and the throughput plateaus.

\begin{table}[]
    \centering
    \begin{tabular}{c|ccc}
        Grid Size & Poisson Data Size (\si{\mega\byte}) & Total Cells & CUDA Threads per Color Stage \\
        \hline
        260x130 & 0.39 & 33800 & 16900 \\
    \end{tabular}
    \todomark{Finish this}
    \caption{Measured datapoints}
    \label{tab:results:speeddata}
\end{table}

% The researcher believes is also the case for the second datapoint, a 380~x~190 grid with 72,200~cells, as the ramp-up time to fully saturate the GPU with threads (64 threads/mp / (32threads/warp * 2 warps/cycle) = 640 cycles) is significant compared to the time required to execute each thread.
The decrease in \todoref{falling due to cache} begins after \SI{2}{\mega\byte}, which is the size of the GTX ~1080 L2 cache (\todocite{1080 whitepaper/specs}).
As the size increases beyond this point the chances of a memory read being present in the cache decreases, so requests to main memory are made more frequently.
Main memory is much slower than L2 cache so the throughput decreases until the data is 2x the cache size (\SI{4}{\mega\byte}), at which point all accesses are to main memory and the throughput plateaus at \SI{10,000}{\mega\op\per\second}.
This behaviour can also be seen with the CPU, an AMD Ryzen 7 1800X with \SI{16}{\mega\byte} of L3 cache, which peaks at \todomark{??MB? likely 16} then drops at \todomark{32???MB} onwards.
\todomark{This should go in evaluation?}
Caching is vital to performance, and this program is no exception.
Adapting the implementation to better utilize cache is crucial to improving simulation speed at scale, and would be excellent to investigate this as future work.

\subsection{Accuracy}\label{sec:Results:Sim:Accuracy}
\input{Ch62Results/figures/acc_table}

Initially accuracy was measured by comparing the Mean Square Error between CPU and CUDA outputs after equivalent simulations (see \cref{tab:results:accuracy_mse}).
Ideally the CUDA and CPU results would be similar, and the MSE would be low (perhaps around $10^{-14}$, as expected in \cref{sec:Comparisons}.)
Instead the results are quite different - the velocity is quite similar after \SI{10}{\second}, but that delta increases to $10^{-6}$ as the simulation progresses.
Pressure is even worse, starting at $10^{-8}$ and going as far as a mean square error of $10^{0} = 1$.
In both cases the divergence between CPU and CUDA increases as more iterations are performed (see \todoref{Velocity log(MSE) and Pressure log(MSE)}.
However, this isn't the whole story.
Measuring the difference between the CPU and CUDA doesn't show which one is more accurate.

The true measure of accuracy for a differential equation solver is the precision of the solution - how close the values come to fulfilling the constraints of the equation.
The original simulation calculated this residual value as part of the Poisson loop, but this was removed for optimization purposes.
\todoref{residual table} presents the residual values as calculated after the simulations completed, showing that the difference between CPU and CUDA in terms of the solver accuracy is very small.
CUDA is slightly less precise, but is at worst within just 5\% of the CPU residual.
This is likely forgivable for the sake of a real-time visualization, but could still be improved in the future.

The large increase in Pressure MSE is explained by \todoref{pressure inflation graph}.
The actual pressure values increase as more Poisson iterations are performed, for both CPU and CUDA.
CUDA's values inflate slightly more slowly, giving each grid cell a high square error, resulting in a large MSE.
When the MSE is calculated \emph{after} subtracting the mean from both datasets, i.e. comparing the relative values, the results are closer to the velocity MSE.
The pressure is only used internally as a relative value, so theoretically this inflation is fine, but in practice if it increases too much it could reach the limits of IEEE-754 floating point \todocite{IEEE 754 floating point} and cause accuracy/precision loss in the solver.
\cite{book:griebel1998numerical} mentioned that ``nonphysical pressure values'' may be the result of noncontinuous starting velocities, and mentioned a method for resolving this (see \cref{ext:PressureValues}). This may prevent the inflation, but it doesn't explain the deviation between the CPU and CUDA.

The CUDA and CPU compilers are configured to handle floating-point numbers slightly differently.
\texttt{nvcc} compiles with the \texttt{--fmad} option turned on by default\todocite{\url{https://docs.nvidia.com/cuda/cuda-compiler-driver-nvcc/index.html\#options-for-steering-gpu-code-generation-fmad}} which ``enables the contraction of floating-point multiplies and adds/subtracts into floating-point multiply-add operations''.
This optimization performs a multiply and an addition at the same time, without a rounding step in between, resulting in a slightly more accurate result\todocite{\url{https://docs.nvidia.com/cuda/floating-point/index.html\#fused-multiply-add-fma}}.
It is disabled for the CPU compiler, to keep the results from the other CPU backends consistent with the original ACA coursework.
While the change is subtle it's present in all CUDA kernels and when applied thousands of times may have a significant effect, leading to the deviation.
In the future this could be verified by compiling the CPU simulation with this option enabled.

\subsection{Efficiency}\label{sec:Results:Sim:Efficiency}
GPU Usage is close to 100\% where possible.
At the tick boundaries, a bubble is unavoidable (see \todoref{sim profile}) where the CPU has to wait for the GPU reductions to finish before calculating the timestep for the next tick and invoking the new calculations.
In practice this bubble is approximately \SI{10}{\micro\second} long, combined with another \SI{10}{\micro\second} long bubble between the Tentative Velocity stage finishing and the Poisson stage beginning.
Each tick at 100 iterations for the initial ACA took \SI{700}{\micro\second}, so this gap accounts for 2.8\% of the runtime.
As the simulation gets larger or the iteration count increases this gap should remain constant, and become even less significant.

\subsection{Memory Leaks}\label{sec:Results:Sim:Mem}
The program is designed to allocate all memory up-front, instead of allocating during a simulation.
This makes memory leaks unlikely, but not impossible.
\texttt{valgrind} was used to test if the program leaked any memory using the CPU backend.
Support for CUDA-based memory in \texttt{valgrind} seemed to be lacking, so it was run with suppressions\todocite{\url{https://forums.developer.nvidia.com/t/valgrind-3-4-suppressions-a-little-howto/7504}} enabled that hid CUDA-related false positives.
It did not find any leaks in the project code, but did find a potential memory leak in the OpenMP implementation\footnote{This may also be a false-positive similar to those from CUDA.}.
\texttt{cuda-memcheck} was then used to find any leaked CUDA memory, and found no potential leaks.
\todomark{examples of results from both programs (check memory\_usage.tex)}

\section{Visualization}
\subsection{Speed}\label{sec:Results:Viz:Speed}
The speed of each individual feature was measured by enabling just that feature, moving to the worst-case for that feature such as enabling auto-range and maximizing onscreeen instances (\cref{tab:results:vizworstcases}), and then reading off the time-to-render from the GUI.
The rendered simulation state was the 660~x~120 ACA input, which is rendered internally at 2x resolution in both directions i.e. 1320~x~240 then composited with the GUI onto a 1600~x~900 window.

\begin{table}[]
    \centering
    \begin{tabular}{c|p{0.7\textwidth}}
        Feature & Worst-Case \\
        \hline
        Quantity-by-Scalar & Auto-range enabled \\
        Quantity-by-Vector & Auto-range enabled, grid spacing set to display maximum number of vectors = 10,000 \\
        Particle System & Particle simulation enabled, emitting maximum amount of particles per frame = 16, simulating and rendering maximum amount of particles = 100,000 \\
    \end{tabular}
    \caption{Testing Scenarios for Visualization Feature Speed.}
    \label{tab:results:vizworstcases}
\end{table}

\input{Ch62Results/figures/viz_speed_table}

The individual features are all faster than the simulation, even when combined, and are significantly shorter than the \SI{16.6}{\milli\second} required for a 60FPS visualization.
Even at scale, the visualization features should not have a significant impact on the visualization speed.

\subsection{Efficiency}\label{sec:Results:Viz:Efficiency}
As for the Simulation, the GPU utilization is 100\% where possible.
Theoretically the visualization work would hide the extra latency from waiting for the reduction to finish, but in practice the visualization work itself waits for around \SI{70}{\micro\second} before starting (\todomark{This is time from semaphore, not time from reduction end, mark that on the graph}).
The next CUDA tick, which has been enqueued well in advance by the CPU, then takes \SI{140}{\micro\second} to start after the previous visualization rendering has finished.
In both cases there was no other GPU work executing for this program, or any other element of this program which could have delayed it, thus it must be due to outside factors e.g. the OS compositing system using the GPU to render the desktop.
\todomark{Vis profile graph.}


\subsection{Memory Leaks}\label{sec:Results:Viz:Memory}
Testing the visualization for memory leaks proved more difficult than for the simulation.
Running \texttt{valgrind} with suppressions enabled found no errors in this program, but found many potential errors (or more likely false-positives) in the SDL2 windowing library and the underlying X windowing system.
\texttt{cuda-memcheck} produced a host of ``Invalid read'' errors, which seemed to originate from CUDA/Vulkan shared memory.
The documentation stated it could not handle DirectX interoperability\todocite{\url{https://docs.nvidia.com/cuda/cuda-memcheck/index.html\#known-issues}}, but said nothing about Vulkan.
It suggested using the \texttt{compute-sanitizer} tool in other situations, which explicitly does not support Vulkan interoperability\todocite{\url{https://docs.nvidia.com/cuda/sanitizer-docs/pdf/ReleaseNotes.pdf}} and produced exactly the same errors.
Given that the errors were identical, it is very likely that both programs do not support Vulkan and should not be trusted.

As a last resort, the Memory Usage statistic from the Nvidia NSight Systems profiler was consulted.
This showed that the CUDA memory usage stayed constant, but had no option to show memory allocated in Vulkan.
As the Vulkan memory is all explicitly allocated at the start with smart resource manager classes it is incredibly unlikely that any Vulkan memory is leaked, but currently there are no tools able to verify this.