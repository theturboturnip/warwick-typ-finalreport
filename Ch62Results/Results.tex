% !TEX root =  ../FinalReport.tex

\newcommand{\npoisson}{N_{Poisson}}

\chapter{Results}\label{sec:Results}
% Some of the tests for non-functional requirements required more in-depth exploration and results gathering.
% This section shows the data gathered to evaluate these requirements.
Some non-functional requirements are based on comparisons to the original program or on tests that require more detailed analysis.
This section shows the data used to evaluate these more complex requirements.
The speed of the simulation is compared the original, and the performance scaling with simulation grid size is measured and justified.
Simulation accuracy vs. the original is evaluated.
GPU Utilization of both the simulation and visualization is measured, and shown to be as high as possible given the design.
Multiple programs are used to try and find memory leaks, and show the program is leak-free.
These results are then used in the Evaluation (\cref{sec:Evaluation}).

\pagebreak
\section{Simulation}
This section investigates four properties of the headless simulation: speed, accuracy, GPU utilization, and memory leaks.
Some of these properties affect the Visualization, as the visualization runs a simulation internally.
This is discussed in \cref{sec:Results:Viz}.

\subsection{Speed}\label{sec:Results:Sim:Speed}

\input{Ch62Results/figures/ticks_vs_time}

Each tick of a simulation performs the same amount of work, so it was hypothesized that the number of ticks per second would be a suitable metric for measuring simulation speed.
To verify this, the original simulation input was simulated with the CUDA and CPU backends for 10, 25, and 50 simulation-seconds with $\npoisson = 100$.
As shown in \cref{fig:results:ticks_vs_time}, elapsed real-time is directly proportional to number of ticks executed with R-values of $R^2_{CPU} = 0.9999$ and $R^2_{CUDA} = 0.9996$.
This proves that ticks-per-second can be used to accurately compare CUDA/CPU performance.

\input{Ch62Results/figures/ticks_per_second_bar_aca}
\input{Ch62Results/figures/ticks_per_second_bar_rel_cpu_aca}

The ticks-per-second for CPU and CUDA are shown in \cref{fig:results:ticks_per_second_bar_aca}, varying $\npoisson \in {100, 200, 300, 1000}$.
As expected the ticks-per-second decreases as more iterations are added, and CUDA is consistently faster than the CPU.
When normalized relative to the CPU speed (\cref{fig:results:ticks_per_second_bar_rel_cpu_aca}), CUDA is shown to consistently have \textasciitilde{}2.6x the ticks-per-second of the CPU, i.e. CUDA is consistently 2.6x faster than the CPU. 
The Poisson stage is likely less sped up than the others, as the speedup for $\npoisson{} = 1000$ (where the Poisson stage dominates) is less than that for lower $\npoisson{}$ values.

\subsubsection{Scaling}
To determine how the implementations scale with grid resolution, a simple simulation with no obstacle squares was performed for resolutions between 260~x~130 and 4096~x~2048.
As the amount of work per simulation tick varies with grid size, each tick is multiplied by the number of grid cells and number of Poisson iterations to determine the number of operations (\si{op}s) executed, and  giga-operations-per-second (\si{\giga\op\per\second}) is used to measure performance.
The increasing size is measured in the total amount of memory (in \si{\mega\byte}) required for a full Poisson red/black iteration, equal to three full size matrices: the pressure matrix, the $\beta$ matrix, and the $rhs$ matrix.
\cref{fig:results:throughput_over_size} shows \si{\giga\op\per\second} for CUDA and CPU against the memory required in \si{\mega\byte}.

\input{Ch62Results/figures/poisson_throughput_vs_size}

Initially, the biggest surprise in \cref{fig:results:throughput_over_size} was the CPU performing better than CUDA between 4-\SI{16}{\mega\byte}.
Other interesting points were the linear increase in CUDA throughput between 0.4-\SI{1.2}{\mega\byte}, and the performance plateau from \SI{4}{\mega\byte} onwards (CUDA) and \SI{32}{\mega\byte} onwards (CPU).
These are due to multiple factors, and are best explained by separating the CUDA graph into stages (\cref{fig:results:throughput_over_size_stages}.

% \todoref{Throughput/Poisson - Inital climb state} shows \si{\mega\op\per\second} increasing almost linearly before plateauing at 1.2-\SI{1.5}{\mega\byte}.
The first stage shows \si{\giga\op\per\second} increasing almost linearly before plateauing at 1.2-\SI{1.5}{\mega\byte}.
This is due to the GPU's parallelization not being fully utilized.
This data was measured on a GTX 1080 which has 2560 CUDA cores separated into 20 Streaming Multiprocessors of 128 cores each\cite{nvidia1080Whitepaper}.
Each Multiprocessor can execute 2048~threads at once\cite{NvidiaCudaOccupancyCalc}, thus the GPU can execute at most 40,960~threads in parallel.
The first datapoint uses a 260~x~130 grid with only 33,800~cells, thus 16,900 threads per Poisson color stage\footnote{The red and black stages only write to the 1/2 of the grid corresponding to their color.}, so the GPU isn't saturated and isn't producing as many outputs as possible per second.
This is also the case for the second datapoint, with 36,100 threads per color stage.
Beyond this point, the GPU is saturated and the throughput plateaus at 2.6x the CPU value.

\begin{table}[]
    \centering
    \begin{tabular}{c|ccc}
        Grid Size & Poisson Data Size (\si{\mega\byte}) & Total Cells & CUDA Threads per Color Stage \\
        \hline
        260x130 & 0.39 & 33,800 & 16,900 \\
        380x190 & 0.83 & 72,200 & 36,100 \\
        460x230 & 1.21 & 105,800 & 52,900 \\
        520x260 & 1.55 & 135,200 & 67,600 \\
        600x300 & 2.06 & 180,000 & 90,000 \\
        640x320 & 2.34 & 204,800 & 102,400 \\
        700x350 & 2.80 & 245,000 & 122,500 \\
        740x370 & 3.13 & 273,800 & 136,900 \\
        840x420 & 4.04 & 352,800 & 176,400 \\
        1180x590 & 7.97 & 696,200 & 348,100 \\
        1680x840 & 16.15 & 1,411,200 & 705,600 \\
        2360x1180 & 31.87 & 2,784,800 & 1,392,400 \\
        4096x2048 & 96.00 & 8,388,608 & 4,194,304 \\
    \end{tabular}
    \caption{Throughput Measurement Points}
    \label{tab:results:speeddata}
\end{table}

% The researcher believes is also the case for the second datapoint, a 380~x~190 grid with 72,200~cells, as the ramp-up time to fully saturate the GPU with threads (64 threads/mp / (32threads/warp * 2 warps/cycle) = 640 cycles) is significant compared to the time required to execute each thread.
% The decrease in \todoref{Throughput/Poisson - falling due to cache} begins after \SI{2}{\mega\byte}, which is the size of the GTX ~1080 L2 cache\cite{nvidia1080Whitepaper}.
The throughput decreases after \SI{2}{\mega\byte}, which is the size of the GTX~1080 L2 cache\cite{nvidia1080Whitepaper}.
As the size increases beyond this point the chances of a memory read being present in the cache decreases, so requests to main memory are made more frequently.
Main memory is much slower than L2 cache so the throughput decreases until the data is 2x the cache size (\SI{4}{\mega\byte}), at which point all accesses are to main memory and the throughput plateaus at \SI{10}{\giga\op\per\second}.
This behaviour can also be seen with the CPU, an AMD Ryzen 7 1800X with \SI{16}{\mega\byte} of L3 cache, which peaks at \SI{16}{\mega\byte} then drops at \SI{32}{\mega\byte} onwards.
Initially when the GPU performance drops it falls below the CPU performance, but once the CPU performance falls the GPU ends up being ~6x faster.

Caching is vital to performance, and this program is no exception.
Adapting the implementation to better utilize cache is crucial to improving simulation speed at scale, and would be excellent to investigate this as future work.

\subsection{Accuracy}\label{sec:Results:Sim:Accuracy}
\input{Ch62Results/figures/acc_table}
\input{Ch62Results/figures/mse_initial}

Initially accuracy was measured by comparing the Mean Square Error between CPU and CUDA outputs after equivalent simulations (see \cref{tab:results:accuracy_mse}).
Ideally the CUDA and CPU results would be similar, and the MSE would be low (perhaps around $10^{-14}$, as expected in \cref{sec:Comparisons}.)
Instead the results are quite different - the velocity is quite similar after \SI{10}{\second}, but that delta increases to $10^{-6}$ as the simulation progresses.
Pressure is even worse, starting at $10^{-8}$ and going as far as a mean square error of $10^{0} = 1$.
In both cases the divergence between CPU and CUDA increases as more iterations are performed (see \cref{fig:results:mse_velocity,fig:results:mse_pressure}).
However, this isn't the whole story.
Measuring the difference between the CPU and CUDA doesn't show which one is more accurate.

\input{Ch62Results/figures/residual_table}

The true measure of accuracy for a differential equation solver is the precision of the solution - how close the values come to fulfilling the constraints of the equation.
The original simulation calculated this residual value as part of the Poisson loop, but this was removed for optimization purposes.
\cref{tab:results:residual} presents the residual values as calculated after the simulations completed, showing that the difference between CPU and CUDA in terms of the solver accuracy is very small.
CUDA is slightly less precise, but is at worst within just 5\% of the CPU residual.
This is likely forgivable for the sake of a real-time visualization, but could still be improved in the future.

\input{Ch62Results/figures/pressure_inflation}
\input{Ch62Results/figures/mse_rel_pressure}

The large increase in Pressure MSE is explained by \cref{fig:results:pressure_inflation}.
The actual pressure values increase as more Poisson iterations are performed, for both CPU and CUDA.
CUDA's values inflate slightly more slowly, giving each grid cell a high square error, resulting in a large MSE.
When the MSE is calculated \emph{after} subtracting the mean from both datasets, i.e. comparing the relative values, the results are closer to the velocity MSE (\cref{fig:results:mse_rel_pressure}).
The pressure is only used internally as a relative value, so theoretically this inflation is fine, but in practice if it increases too much it could reach the limits of IEEE-754 floating point\cite{IEEEFloat75419} and cause accuracy/precision loss in the solver.
\cite{book:griebel1998numerical} mentioned that ``nonphysical pressure values'' may be the result of noncontinuous starting velocities, and mentioned a method for resolving this (see \cref{ext:PressureValues}). This may prevent the inflation, but it doesn't explain the deviation between the CPU and CUDA.

The CUDA and CPU compilers are configured to handle floating-point numbers slightly differently.
\shell{nvcc} compiles with the \shell{--fmad} option turned on by default\cite{NvccFmad} which ``enables the contraction of floating-point multiplies and adds/subtracts into floating-point multiply-add operations''.
This optimization performs a multiply and an addition at the same time, without a rounding step in between, resulting in a slightly more accurate result\cite{NvidiaFMAJustification}.
It is disabled for the CPU compiler, to keep the results from the other CPU backends consistent with the original simulation.
While the change is subtle it's present in all CUDA kernels and when applied thousands of times may have a significant effect, leading to the deviation.
In the future this could be verified by compiling the CPU simulation with this option enabled.

\subsection{GPU Utilization}\label{sec:Results:Sim:Efficiency}
\input{Ch62Results/figures/profile_sim}
GPU Utilization is close to 100\% where possible.
During the Poisson iterations the GPU utilization is 100\% thanks to the CUDA Graph optimization.
At the tick boundaries, a bubble is unavoidable (see \cref{fig:results:sim_profile_bubble}) where the CPU has to wait for the GPU reductions to finish before calculating the timestep for the next tick and invoking the next kernels.
In practice this bubble is approximately \SI{10}{\micro\second} long, combined with another \SI{10}{\micro\second} long bubble between the Tentative Velocity stage finishing and the Poisson stage beginning.
Each tick at 100 iterations for this simulation took \SI{700}{\micro\second}, so this gap accounts for 2.8\% of the runtime.
As the simulation gets larger or the iteration count increases this gap should remain constant, and become even less significant.
Computing the timestep entirely on the GPU, instead of sending data back and forth from the CPU, could allow the kernels to be enqueued earlier and avoid the GPU bubble.

\subsection{Memory Leaks}\label{sec:Results:Sim:Mem}
The program is designed to allocate all memory up-front, instead of allocating during a simulation.
This makes memory leaks unlikely, but not impossible.
\shell{valgrind} was used to test if the program leaked any memory using the CPU backend.
Support for CUDA-based memory in \shell{valgrind} seemed to be lacking, so it was run with suppressions enabled that hid CUDA-related false positives.
% \todocite{\url{https://forums.developer.nvidia.com/t/valgrind-3-4-suppressions-a-little-howto/7504}}
It did not find any leaks in the project code, but did find a potential memory leak in the OpenMP implementation\footnote{This may also be a false-positive similar to those from CUDA.}.
\shell{cuda-memcheck} was then used to find any leaked CUDA memory, and found no potential leaks.
% \todomark{examples of results from both programs (check memory\_usage.tex)}

\pagebreak
\section{Visualization}\label{sec:Results:Viz}
This section investigates three properties of the real-time visualization: speed, GPU utilization, and memory leaks.
While these properties do overlap with the simulation, this section focuses only on the properties of the visualization itself.
% Some of these properties affect the Visualization, as the Visualization runs the same code internally.
% This is discussed in \cref{sec:Results:Viz}.

\subsection{Speed}\label{sec:Results:Viz:Speed}
The speed of each individual feature was measured by enabling just that feature, moving to the worst-case for that feature such as enabling auto-range and maximizing onscreeen instances (\cref{tab:results:vizworstcases}), and then reading off the time-to-render from the GUI.
The rendered simulation state was the 660~x~120 original simulation input, which is rendered internally at 2x resolution in both directions i.e. 1320~x~240 then composited with the GUI onto a 1600~x~900 window.

\begin{table}[]
    \centering
    \begin{tabular}{c|p{0.7\textwidth}}
        Feature & Worst-Case \\
        \hline
        Quantity-by-Scalar & Auto-range enabled \\
        Quantity-by-Vector & Auto-range enabled, grid spacing set to display maximum number of vectors = 10,000 \\
        Particle System & Particle simulation enabled, emitting maximum amount of particles per frame = 16, simulating and rendering maximum amount of particles = 100,000 \\
    \end{tabular}
    \caption{Testing Scenarios for Visualization Feature Speed.}
    \label{tab:results:vizworstcases}
\end{table}

\input{Ch62Results/figures/viz_speed_table}

The individual features are all faster than the simulation, even when combined, and are significantly shorter than the \SI{16.6}{\milli\second} required for a 60FPS visualization.
Even at scale, the visualization features should not have a significant impact on the visualization speed.

\subsection{GPU Utilization}\label{sec:Results:Viz:Efficiency}
\input{Ch62Results/figures/profile_viz}
As for the Simulation, the GPU utilization is 100\% where possible.
Theoretically the visualization work would hide the extra latency from waiting for the reduction to finish, but in practice the visualization work itself waits for around \SI{70}{\micro\second} after the semaphore is raised before starting (\cref{fig:results:viz_profile_bubble}).
The next CUDA tick, which has been enqueued well in advance by the CPU, then takes \SI{140}{\micro\second} to start after the previous visualization rendering has finished.
In both cases there was no other GPU work executing for this program, or any other element of this program which could have delayed it, thus it must be due to outside factors e.g. the OS compositing system using the GPU to render the desktop.


\subsection{Memory Leaks}\label{sec:Results:Viz:Memory}
Testing the visualization for memory leaks proved more difficult than for the simulation.
Running \shell{valgrind} with suppressions enabled found no errors in this program, but found many potential errors (or more likely false-positives) in the SDL2 windowing library and the underlying X windowing system.
\shell{cuda-memcheck} produced a host of ``Invalid read'' errors, which seemed to originate from CUDA/Vulkan shared memory.
The documentation stated it could not handle DirectX interoperability\cite{NvidiaCudaMemcheckIssues}, but said nothing about Vulkan.
It suggested using the \shell{compute-sanitizer} tool in other situations, which explicitly does not support Vulkan interoperability\cite{NvidiaComputeSanitizerRelease} and produced exactly the same errors.
Given that the errors were identical, it is very likely that both programs do not support Vulkan and should not be trusted.

As a last resort, the Memory Usage statistic from the Nvidia NSight Systems profiler was consulted.
This showed that the CUDA memory usage stayed constant, but had no option to show memory allocated in Vulkan.
Vulkan memory should all be explicitly allocated at the start with smart resource manager classes, so it is incredibly unlikely that any Vulkan memory is leaked, but currently there are no tools able to verify this.